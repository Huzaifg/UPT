Code and instructions are coming soon.


[[Project Page](https://ml-jku.github.io/UPT)] [[Paper (arxiv)](https://arxiv.org/abs/2402.12365)] [[BibTeX](https://github.com/ml-jku/UPT#citation)]



# Train your own models

Instructions to setup the codebase on your own environment are provided in 
[SETUP_CODE](https://github.com/ml-jku/MIM-Refiner/blob/main/SETUP_CODE.md), 
[SETUP_DATA](https://github.com/ml-jku/MIM-Refiner/blob/main/SETUP_DATA.md).

Configurations to train, evaluate or analyze models can be found [here](https://github.com/ml-jku/MIM-Refiner/tree/main/src/yamls).

# Citation

If you like our work, please consider giving it a star :star: and cite us

```
@article{alkin2024upt,
      title={Universal Physics Transformers}, 
      author={Benedikt Alkin and Andreas FÃ¼rst and Simon Schmid and Lukas Gruber and Markus Holzleitner and Johannes Brandstetter},
      journal={arXiv preprint arXiv:2402.12365},
      year={2024}
}
```